{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is SPARK installed on the VM? Find and import the SPARK library\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step: create the SPARK context. Without this, SPARK functionality cannot be used\n",
    "sc = pyspark.SparkContext(appName=\"SPARkBasics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step will be creating an RDD (Resilient distributed dataset)\n",
    "# RDDs can be created from many sources: by reading data from a file, from a Python list, etc.\n",
    "# The following statement created an RDD from a Python list\n",
    "\n",
    "a_list = [1,3,5,7,9,11,13,15,17,19,21]\n",
    "\n",
    "# 'parallelize' function from the class sparkContext is used to create RDDs from in memory objects\n",
    "rdd1 = sc.parallelize(a_list) \n",
    "\n",
    "print(type(a_list),\"\\n\",type(rdd1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the RDD, we can invoke the RDD functions on it (transformations and actions)\n",
    "print(rdd1.count())\n",
    "print(rdd1.first())\n",
    "print(rdd1.take(3))\n",
    "print(rdd1.top(3))\n",
    "print(rdd1.sum())\n",
    "print(rdd1.mean())\n",
    "print(rdd1.min())\n",
    "print(rdd1.max())\n",
    "print(rdd1.sampleStdev())\n",
    "\n",
    "# ... and many more\n",
    "# Refer: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An RDD is distributed. If we want the RDD to get combined into a Python list (for printing, etc.) ... we use 'collect'\n",
    "list_from_rdd = rdd1.collect()\n",
    "print(list_from_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an RDD into a text file\n",
    "# Note : The following function will create a 'directory' with the name 'rdd_files' ...\n",
    "# ... inside the directory there will be a number of files named 'part-XXXXX' where XXXXX will be 00000, 00001, and so oon\n",
    "# The number of such 'part' files will depend on the number of 'partitions' of the RDD \n",
    "rdd1.saveAsTextFile(\"rdd_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing some general information about the 'sc' \n",
    "print(sc.applicationId)\n",
    "print(sc.defaultMinPartitions)\n",
    "print(sc.defaultParallelism)\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are done working with SPARK, so release the SPARK context created at the beginning\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
