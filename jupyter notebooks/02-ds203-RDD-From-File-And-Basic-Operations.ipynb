{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need this for splitting a line into words using regular expressions\n",
    "import re \n",
    "\n",
    "# Find the SPARK ibrary, so that it can be imported into Jupyter Notebook\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "# Link in the Python version of the SPARK library\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step - creating the SPARK Context\n",
    "sc = pyspark.SparkContext(appName=\"RDDOperations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the SparkContext, and its function 'textFile', create an RDD 'rdd1' by reading a text file\n",
    "# The functions defined in the class 'SparkContext' are documented at ...\n",
    "# https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.html\n",
    "\n",
    "rdd1 = sc.textFile(\"/home/hduser/data/pg20417.txt\")\n",
    "print(type(rdd1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class RDD has a number of useful functions for performing various 'transformations' and 'actions' on the RDD\n",
    "# Refer to https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.RDD.html\n",
    "print(\"Number of lines in the file: \",rdd1.count())\n",
    "print(\"\\nThe first 2 lines line: \", rdd1.take(2))\n",
    "print(\"\\nPrint only the first line: \", rdd1.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a 'word count' for the file by using the 'Map - Reduce' approach ... \n",
    "# First step: Split each line into its constituent words, and create a collection of these words using 'flatmap' function of RDD\n",
    "# Splitting is accomplished using a lambda function ... and regular expressions\n",
    "the_words = rdd1.flatMap(lambda x: re.split(\"[ .,!()\\'\\\"\\n\\r?:;_\\-|{}\\[\\]\\\\\\/]\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 words, and the total count of words resulting from the above operation \n",
    "print(the_words.take(5))\n",
    "print(the_words.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin the map-reduce process to create a count of each word\n",
    "# First step: Each word is converted into a 'key-value' pair\n",
    "# A new RDD 'words_kv' is created. It is an RDD of the Python type 'dictionary'\n",
    "words_kv = the_words.map(lambda x : (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 5 (k,v) pairs\n",
    "print(words_kv.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second step: We use the 'CountByKey' function of RDD to 'reduce' the (k,v) pairs\n",
    "# The resulting 'word_count' is a Python 'dictionary'\n",
    "word_count = words_kv.countByKey()\n",
    "print(type(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 key-value pairs\n",
    "count = 0\n",
    "for key, value in word_count.items():\n",
    "    print(key, ':', value)\n",
    "    count += 1\n",
    "    if count >= 20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the entire dictionary into a csv file\n",
    "import csv\n",
    "\n",
    "# Write dictionary to a CSV file\n",
    "with open('word_count.csv', 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    for key, value in word_count.items():\n",
    "        csv_writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we are done with using SPARK, we should NOT forget to 'stop' the SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
